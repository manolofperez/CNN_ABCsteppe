{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CÃ³pia de Train_CNN_Piloso",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Script and outputs for training a CNN to peform parameter estimation in the species *Euphorbia segueriana* (scripts for the remaining species are similar).** \n",
        "From the manuscript Kirschner & Perez et al. \"Congruent evolutionary responses of European steppe biota to late Quaternary climate change: insights from convolutional neural network-based demographic modeling\"."
      ],
      "metadata": {
        "id": "ELhRS0fu2b6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Import all required modules.\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import keras\n",
        "import random\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.constraints import max_norm\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.convolutional import Conv1D, Conv2D\n",
        "from keras.layers.pooling import AveragePooling1D, AveragePooling2D\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "import time\n",
        "from random import shuffle, choice\n",
        "\n",
        "# Define parameters for the CNN run.\n",
        "batch_size = 500\n",
        "epochs = 500\n",
        "\n",
        "# Define a function to read the parameters file.\n",
        "def readDemogParams(demogParamPath):\n",
        "    params = []\n",
        "    first = True\n",
        "    with open(demogParamPath) as demogParamFile:\n",
        "        for line in demogParamFile:\n",
        "            params.append([float(x) for x in line.strip().split()])\n",
        "    return params\n",
        "\t\n",
        "# Define the CNN architecture.\n",
        "def create_cnn(xtest):\n",
        "\t#use different filter sizes.\n",
        "\tfilters = [3,5,20,50]\n",
        "\tinputShape = (imgRows, imgCols)\n",
        "\tinputs = Input(shape=inputShape)\n",
        "\tx = inputs\n",
        "\tx = Conv1D(250, kernel_size=2, activation='relu',input_shape=(xtest.shape[1], xtest.shape[2]))(x)\n",
        "\tx = BatchNormalization()(x)\n",
        "\tfor f in range(len(filters)):\n",
        "\t\tx = Conv1D(20, kernel_size=filters[f], activation='relu',kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(x)\n",
        "\t\tx = BatchNormalization()(x)\n",
        "\tx = Conv1D(125, kernel_size=2, activation='relu',kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(x)\n",
        "\tx = BatchNormalization()(x)\n",
        "\tx = Conv1D(125, kernel_size=2, activation='relu',kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(x)\n",
        "\tx = AveragePooling1D(pool_size=2)(x)\n",
        "\tx = Flatten()(x)\n",
        "\tx = Dense(125, activation='relu',kernel_regularizer=l2(1e-3), bias_regularizer=l2(1e-3))(x)\n",
        "\tx = Dropout(0.5)(x)\n",
        "\tx = Dense(125, activation='relu',kernel_regularizer=l2(1e-3), bias_regularizer=l2(1e-3))(x)\n",
        "\tx = Dropout(0.5)(x)\n",
        "\t# The last layer is a dense according to the number of parameters.\n",
        "\tx = Dense(numParams)(x)\n",
        "\n",
        "\t# Construct the CNN.\n",
        "\tmodel = Model(inputs, x)\n",
        "\t# Return the CNN.\n",
        "\treturn model"
      ],
      "outputs": [],
      "metadata": {
        "id": "Zgc_VfbydhlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the network with 10,000 simulations from the selected model**\n",
        "Use the simulated dataset to train the network, by splitting the data with 75% of simulations for training and 25% for validation."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load Numpy arrays containing simulations.\n",
        "x = np.load(\"./trainingSims/simModel3.npy\",mmap_mode='r')\n",
        "x = np.array(x)\n",
        "imgRows, imgCols = x.shape[1:]\n",
        "\n",
        "# Load parameters.\n",
        "demogParams = readDemogParams('parameters3.txt')\n",
        "y = np.array(demogParams)\n",
        "numParams=y.shape[1]\n",
        "\n",
        "#delete temporary files to free memory.\n",
        "del (demogParams)\n",
        "\n",
        "# Print label and simulations length, these should be the same.\n",
        "print (len(x), len(y))\n",
        "\n",
        "# Shuffle the arrays for training, keeping the labels in the same order.\n",
        "shf = list(range(len(x)))\n",
        "shuffle(shf)\n",
        "y = y[shf]\n",
        "x = x[shf]\n",
        "\n",
        "# Convert the reference allele to -1.\n",
        "x[x == 0] = -1\n",
        "\n",
        "#Add missing data (coded as 0s) to the simulated matrices (with a percentage according to the empirical data - 15% in E. segueriana).\n",
        "missD = int(x.shape[1]*x.shape[2]*.15)\n",
        "for i in range(x.shape[0]):\n",
        "  for m in range(missD):\n",
        "    j = random.randint(0, x.shape[1] - 1)\n",
        "    k = random.randint(0, x.shape[2] - 1)\n",
        "    x[i][j][k] = 0\n",
        "del(missD)\n",
        "\n",
        "# Standardize parameters before training.\n",
        "yMeans=np.mean(y, axis=0)\n",
        "yStds=np.std(y, axis=0)\n",
        "y = (y-yMeans)/yStds\n",
        "\n",
        "# Print parameters means and std deviations.\n",
        "print (yMeans)\n",
        "print (yStds)\n",
        "\n",
        "# Separate train (75%) and validate (25%) sets.\n",
        "xtrain, xtest = x[int(len(y)*.25):], x[:int(len(y)*.25)]\n",
        "ytrain, ytest = y[int(len(y)*.25):], y[:int(len(y)*.25)]\n",
        "del(x)\n",
        "\n",
        "# Create the CNN network.\n",
        "cnn = create_cnn(xtest)\n",
        "\n",
        "# Compile the CNN.\n",
        "cnn.compile(loss='mean_squared_error',\n",
        "\t              optimizer='Adam')\n",
        "\n",
        "# Check the architecture.\n",
        "cnn.summary()\n",
        "\n",
        "# Run the CNN with early stopping and reducing the learning rate after reaching a plateau. Save the model with the best val_accuracy.\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
        "\n",
        "cnn.fit(xtrain, ytrain, batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(xtest, ytest),callbacks=[earlyStopping])\n",
        "\n",
        "# Save the model.\n",
        "with open('Trained_Params_10KSims.acc.mod', \"w+\") as modFile:\n",
        "    modFile.write(cnn.to_json())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000 30000\n",
            "[2.86208248e-01 1.34735301e+06 6.16353922e+04 5.97829560e+03 1.20255566e+05 2.61763230e+00 5.27521489e+01 7.49064630e-01 2.54971359e+00 2.51017993e+00 2.52481896e+00 2.49583994e+00]\n",
            "[1.09124240e-01 6.05808853e+05 2.81277033e+04 3.48151789e+03 4.58505224e+04 1.36959349e+00 2.73631598e+01 1.45081116e-01 1.43927692e+00 1.44919707e+00 1.43801672e+00 1.44210643e+00]\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1000, 270)]       0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 999, 250)          135250    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 999, 250)          1000      \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 997, 20)           15020     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 997, 20)           80        \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 993, 20)           2020      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 993, 20)           80        \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 974, 20)           8020      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 974, 20)           80        \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 925, 20)           20020     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 925, 20)           80        \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 924, 125)          5125      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 924, 125)          500       \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 923, 125)          31375     \n",
            "_________________________________________________________________\n",
            "average_pooling1d (AveragePo (None, 461, 125)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 57625)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 125)               7203250   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 125)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 125)               15750     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 125)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 12)                1512      \n",
            "=================================================================\n",
            "Total params: 7,439,162\n",
            "Trainable params: 7,438,252\n",
            "Non-trainable params: 910\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/500\n",
            "12/12 [==============================] - ETA: 0s - loss: 14.5547\n",
            "12/12 [==============================] - 85s 7s/step - loss: 14.5547 - val_loss: 1.4925\n",
            "Epoch 2/500\n",
            "12/12 [==============================] - 67s 6s/step - loss: 1.5103 - val_loss: 1.5363\n",
            "Epoch 3/500\n",
            "12/12 [==============================] - 67s 6s/step - loss: 1.5296 - val_loss: 1.5342\n",
            "Epoch 4/500\n",
            "12/12 [==============================] - 69s 6s/step - loss: 1.5186 - val_loss: 1.5159\n",
            "Epoch 5/500\n",
            "12/12 [==============================] - 67s 6s/step - loss: 1.4979 - val_loss: 1.4935\n",
            "Epoch 6/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.4753 - val_loss: 1.4711\n",
            "Epoch 7/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.4533 - val_loss: 1.4497\n",
            "Epoch 8/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.4326 - val_loss: 1.4297\n",
            "Epoch 9/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.4132 - val_loss: 1.4111\n",
            "Epoch 10/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.3951 - val_loss: 1.3937\n",
            "Epoch 11/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.3783 - val_loss: 1.3774\n",
            "Epoch 12/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.3625 - val_loss: 1.3622\n",
            "Epoch 13/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.3478 - val_loss: 1.3480\n",
            "Epoch 14/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.3340 - val_loss: 1.3347\n",
            "Epoch 15/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.3210 - val_loss: 1.3221\n",
            "Epoch 16/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.3088 - val_loss: 1.3104\n",
            "Epoch 17/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.2974 - val_loss: 1.2993\n",
            "Epoch 18/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.2866 - val_loss: 1.2889\n",
            "Epoch 19/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.2765 - val_loss: 1.2790\n",
            "Epoch 20/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.2669 - val_loss: 1.2698\n",
            "Epoch 21/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.2579 - val_loss: 1.2610\n",
            "Epoch 22/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.2493 - val_loss: 1.2527\n",
            "Epoch 23/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.2413 - val_loss: 1.2449\n",
            "Epoch 24/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.2336 - val_loss: 1.2375\n",
            "Epoch 25/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.2264 - val_loss: 1.2305\n",
            "Epoch 26/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.2196 - val_loss: 1.2238\n",
            "Epoch 27/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.2131 - val_loss: 1.2175\n",
            "Epoch 28/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.2069 - val_loss: 1.2115\n",
            "Epoch 29/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.2011 - val_loss: 1.2059\n",
            "Epoch 30/500\n",
            "12/12 [==============================] - 86s 7s/step - loss: 1.1955 - val_loss: 1.2005\n",
            "Epoch 31/500\n",
            "12/12 [==============================] - 92s 8s/step - loss: 1.1902 - val_loss: 1.1953\n",
            "Epoch 32/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1852 - val_loss: 1.1904\n",
            "Epoch 33/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1804 - val_loss: 1.1858\n",
            "Epoch 34/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.1759 - val_loss: 1.1814\n",
            "Epoch 35/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1716 - val_loss: 1.1771\n",
            "Epoch 36/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1674 - val_loss: 1.1731\n",
            "Epoch 37/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.1635 - val_loss: 1.1693\n",
            "Epoch 38/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.1597 - val_loss: 1.1656\n",
            "Epoch 39/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1562 - val_loss: 1.1621\n",
            "Epoch 40/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.1527 - val_loss: 1.1588\n",
            "Epoch 41/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1494 - val_loss: 1.1556\n",
            "Epoch 42/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.1463 - val_loss: 1.1525\n",
            "Epoch 43/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1433 - val_loss: 1.1496\n",
            "Epoch 44/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1404 - val_loss: 1.1467\n",
            "Epoch 45/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1377 - val_loss: 1.1440\n",
            "Epoch 46/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.1350 - val_loss: 1.1414\n",
            "Epoch 47/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.1325 - val_loss: 1.1390\n",
            "Epoch 48/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1300 - val_loss: 1.1366\n",
            "Epoch 49/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1277 - val_loss: 1.1343\n",
            "Epoch 50/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1254 - val_loss: 1.1321\n",
            "Epoch 51/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.1233 - val_loss: 1.1299\n",
            "Epoch 52/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1212 - val_loss: 1.1279\n",
            "Epoch 53/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.1192 - val_loss: 1.1259\n",
            "Epoch 54/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.1172 - val_loss: 1.1240\n",
            "Epoch 55/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1153 - val_loss: 1.1222\n",
            "Epoch 56/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.1135 - val_loss: 1.1204\n",
            "Epoch 57/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.1118 - val_loss: 1.1187\n",
            "Epoch 58/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1101 - val_loss: 1.1170\n",
            "Epoch 59/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1085 - val_loss: 1.1154\n",
            "Epoch 60/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1069 - val_loss: 1.1139\n",
            "Epoch 61/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1054 - val_loss: 1.1124\n",
            "Epoch 62/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.1039 - val_loss: 1.1110\n",
            "Epoch 63/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.1025 - val_loss: 1.1095\n",
            "Epoch 64/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.1011 - val_loss: 1.1082\n",
            "Epoch 65/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0997 - val_loss: 1.1069\n",
            "Epoch 66/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0984 - val_loss: 1.1056\n",
            "Epoch 67/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0972 - val_loss: 1.1043\n",
            "Epoch 68/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0959 - val_loss: 1.1031\n",
            "Epoch 69/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0947 - val_loss: 1.1019\n",
            "Epoch 70/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0936 - val_loss: 1.1008\n",
            "Epoch 71/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0924 - val_loss: 1.0996\n",
            "Epoch 72/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0913 - val_loss: 1.0986\n",
            "Epoch 73/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0902 - val_loss: 1.0975\n",
            "Epoch 74/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0892 - val_loss: 1.0965\n",
            "Epoch 75/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0882 - val_loss: 1.0954\n",
            "Epoch 76/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0872 - val_loss: 1.0945\n",
            "Epoch 77/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0862 - val_loss: 1.0935\n",
            "Epoch 78/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0852 - val_loss: 1.0926\n",
            "Epoch 79/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0843 - val_loss: 1.0916\n",
            "Epoch 80/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0834 - val_loss: 1.0907\n",
            "Epoch 81/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0825 - val_loss: 1.0898\n",
            "Epoch 82/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0816 - val_loss: 1.0890\n",
            "Epoch 83/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0808 - val_loss: 1.0881\n",
            "Epoch 84/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0799 - val_loss: 1.0873\n",
            "Epoch 85/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0791 - val_loss: 1.0865\n",
            "Epoch 86/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0783 - val_loss: 1.0857\n",
            "Epoch 87/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0775 - val_loss: 1.0849\n",
            "Epoch 88/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0767 - val_loss: 1.0841\n",
            "Epoch 89/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.0759 - val_loss: 1.0834\n",
            "Epoch 90/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0752 - val_loss: 1.0826\n",
            "Epoch 91/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0745 - val_loss: 1.0819\n",
            "Epoch 92/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0737 - val_loss: 1.0812\n",
            "Epoch 93/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0730 - val_loss: 1.0805\n",
            "Epoch 94/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0723 - val_loss: 1.0798\n",
            "Epoch 95/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0716 - val_loss: 1.0791\n",
            "Epoch 96/500\n",
            "12/12 [==============================] - 81s 7s/step - loss: 1.0709 - val_loss: 1.0784\n",
            "Epoch 97/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0703 - val_loss: 1.0777\n",
            "Epoch 98/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0696 - val_loss: 1.0771\n",
            "Epoch 99/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0689 - val_loss: 1.0764\n",
            "Epoch 100/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0683 - val_loss: 1.0758\n",
            "Epoch 101/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0677 - val_loss: 1.0751\n",
            "Epoch 102/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0670 - val_loss: 1.0745\n",
            "Epoch 103/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0664 - val_loss: 1.0739\n",
            "Epoch 104/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0658 - val_loss: 1.0733\n",
            "Epoch 105/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0652 - val_loss: 1.0727\n",
            "Epoch 106/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0646 - val_loss: 1.0721\n",
            "Epoch 107/500\n",
            "12/12 [==============================] - 81s 7s/step - loss: 1.0640 - val_loss: 1.0715\n",
            "Epoch 108/500\n",
            "12/12 [==============================] - 81s 7s/step - loss: 1.0634 - val_loss: 1.0709\n",
            "Epoch 109/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0628 - val_loss: 1.0704\n",
            "Epoch 110/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0623 - val_loss: 1.0698\n",
            "Epoch 111/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.0617 - val_loss: 1.0692\n",
            "Epoch 112/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0612 - val_loss: 1.0687\n",
            "Epoch 113/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0606 - val_loss: 1.0681\n",
            "Epoch 114/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0601 - val_loss: 1.0676\n",
            "Epoch 115/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0595 - val_loss: 1.0671\n",
            "Epoch 116/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0590 - val_loss: 1.0665\n",
            "Epoch 117/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0585 - val_loss: 1.0660\n",
            "Epoch 118/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0579 - val_loss: 1.0655\n",
            "Epoch 119/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0574 - val_loss: 1.0650\n",
            "Epoch 120/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0569 - val_loss: 1.0644\n",
            "Epoch 121/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0564 - val_loss: 1.0639\n",
            "Epoch 122/500\n",
            "12/12 [==============================] - 81s 7s/step - loss: 1.0559 - val_loss: 1.0634\n",
            "Epoch 123/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0554 - val_loss: 1.0629\n",
            "Epoch 124/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.0549 - val_loss: 1.0624\n",
            "Epoch 125/500\n",
            "12/12 [==============================] - 80s 7s/step - loss: 1.0544 - val_loss: 1.0619\n",
            "Epoch 126/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0539 - val_loss: 1.0615\n",
            "Epoch 127/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0534 - val_loss: 1.0610\n",
            "Epoch 128/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0529 - val_loss: 1.0605\n",
            "Epoch 129/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0525 - val_loss: 1.0600\n",
            "Epoch 130/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0520 - val_loss: 1.0596\n",
            "Epoch 131/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0515 - val_loss: 1.0591\n",
            "Epoch 132/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0511 - val_loss: 1.0586\n",
            "Epoch 133/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0506 - val_loss: 1.0582\n",
            "Epoch 134/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0502 - val_loss: 1.0577\n",
            "Epoch 135/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0497 - val_loss: 1.0573\n",
            "Epoch 136/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.0493 - val_loss: 1.0568\n",
            "Epoch 137/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0488 - val_loss: 1.0564\n",
            "Epoch 138/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0484 - val_loss: 1.0560\n",
            "Epoch 139/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0479 - val_loss: 1.0555\n",
            "Epoch 140/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.0475 - val_loss: 1.0551\n",
            "Epoch 141/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0471 - val_loss: 1.0547\n",
            "Epoch 142/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0467 - val_loss: 1.0543\n",
            "Epoch 143/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.0462 - val_loss: 1.0538\n",
            "Epoch 144/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0458 - val_loss: 1.0534\n",
            "Epoch 145/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0454 - val_loss: 1.0530\n",
            "Epoch 146/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.0450 - val_loss: 1.0526\n",
            "Epoch 147/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0446 - val_loss: 1.0522\n",
            "Epoch 148/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0442 - val_loss: 1.0518\n",
            "Epoch 149/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0438 - val_loss: 1.0514\n",
            "Epoch 150/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0434 - val_loss: 1.0510\n",
            "Epoch 151/500\n",
            "12/12 [==============================] - 81s 7s/step - loss: 1.0430 - val_loss: 1.0506\n",
            "Epoch 152/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0426 - val_loss: 1.0502\n",
            "Epoch 153/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0422 - val_loss: 1.0498\n",
            "Epoch 154/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0418 - val_loss: 1.0495\n",
            "Epoch 155/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0415 - val_loss: 1.0491\n",
            "Epoch 156/500\n",
            "12/12 [==============================] - 79s 7s/step - loss: 1.0411 - val_loss: 1.0487\n",
            "Epoch 157/500\n",
            "12/12 [==============================] - 78s 6s/step - loss: 1.0407 - val_loss: 1.0483\n",
            "Epoch 158/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0403 - val_loss: 1.0479\n",
            "Epoch 159/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0400 - val_loss: 1.0476\n",
            "Epoch 160/500\n",
            "12/12 [==============================] - 78s 7s/step - loss: 1.0396 - val_loss: 1.0472\n",
            "Epoch 161/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0392 - val_loss: 1.0469\n",
            "Epoch 162/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0389 - val_loss: 1.0465\n",
            "Epoch 163/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0385 - val_loss: 1.0462\n",
            "Epoch 164/500\n",
            "12/12 [==============================] - 76s 6s/step - loss: 1.0382 - val_loss: 1.0458\n",
            "Epoch 165/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0378 - val_loss: 1.0455\n",
            "Epoch 166/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0375 - val_loss: 1.0451\n",
            "Epoch 167/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0371 - val_loss: 1.0448\n",
            "Epoch 168/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0368 - val_loss: 1.0444\n",
            "Epoch 169/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0365 - val_loss: 1.0441\n",
            "Epoch 170/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0361 - val_loss: 1.0438\n",
            "Epoch 171/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0358 - val_loss: 1.0434\n",
            "Epoch 172/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0355 - val_loss: 1.0431\n",
            "Epoch 173/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0351 - val_loss: 1.0428\n",
            "Epoch 174/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0348 - val_loss: 1.0425\n",
            "Epoch 175/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0345 - val_loss: 1.0422\n",
            "Epoch 176/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0342 - val_loss: 1.0418\n",
            "Epoch 177/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0339 - val_loss: 1.0415\n",
            "Epoch 178/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0336 - val_loss: 1.0412\n",
            "Epoch 179/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0333 - val_loss: 1.0409\n",
            "Epoch 180/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0330 - val_loss: 1.0406\n",
            "Epoch 181/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0327 - val_loss: 1.0403\n",
            "Epoch 182/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0324 - val_loss: 1.0400\n",
            "Epoch 183/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0321 - val_loss: 1.0397\n",
            "Epoch 184/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0318 - val_loss: 1.0394\n",
            "Epoch 185/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0315 - val_loss: 1.0392\n",
            "Epoch 186/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0312 - val_loss: 1.0389\n",
            "Epoch 187/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0309 - val_loss: 1.0386\n",
            "Epoch 188/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0306 - val_loss: 1.0383\n",
            "Epoch 189/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0304 - val_loss: 1.0380\n",
            "Epoch 190/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0301 - val_loss: 1.0378\n",
            "Epoch 191/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0298 - val_loss: 1.0375\n",
            "Epoch 192/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0295 - val_loss: 1.0372\n",
            "Epoch 193/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0293 - val_loss: 1.0369\n",
            "Epoch 194/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0290 - val_loss: 1.0367\n",
            "Epoch 195/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0288 - val_loss: 1.0364\n",
            "Epoch 196/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0285 - val_loss: 1.0362\n",
            "Epoch 197/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0282 - val_loss: 1.0359\n",
            "Epoch 198/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0280 - val_loss: 1.0357\n",
            "Epoch 199/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0277 - val_loss: 1.0354\n",
            "Epoch 200/500\n",
            "12/12 [==============================] - 56s 5s/step - loss: 1.0275 - val_loss: 1.0352\n",
            "Epoch 201/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0272 - val_loss: 1.0349\n",
            "Epoch 202/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0270 - val_loss: 1.0347\n",
            "Epoch 203/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0267 - val_loss: 1.0344\n",
            "Epoch 204/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0265 - val_loss: 1.0342\n",
            "Epoch 205/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0263 - val_loss: 1.0340\n",
            "Epoch 206/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0260 - val_loss: 1.0337\n",
            "Epoch 207/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0258 - val_loss: 1.0335\n",
            "Epoch 208/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0256 - val_loss: 1.0333\n",
            "Epoch 209/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0254 - val_loss: 1.0331\n",
            "Epoch 210/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0251 - val_loss: 1.0328\n",
            "Epoch 211/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0249 - val_loss: 1.0326\n",
            "Epoch 212/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0247 - val_loss: 1.0324\n",
            "Epoch 213/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0245 - val_loss: 1.0322\n",
            "Epoch 214/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0243 - val_loss: 1.0320\n",
            "Epoch 215/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0240 - val_loss: 1.0318\n",
            "Epoch 216/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0238 - val_loss: 1.0315\n",
            "Epoch 217/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0236 - val_loss: 1.0313\n",
            "Epoch 218/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0234 - val_loss: 1.0312\n",
            "Epoch 219/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0232 - val_loss: 1.0309\n",
            "Epoch 220/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0230 - val_loss: 1.0307\n",
            "Epoch 221/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0228 - val_loss: 1.0305\n",
            "Epoch 222/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0226 - val_loss: 1.0303\n",
            "Epoch 223/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0224 - val_loss: 1.0301\n",
            "Epoch 224/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0222 - val_loss: 1.0300\n",
            "Epoch 225/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0221 - val_loss: 1.0298\n",
            "Epoch 226/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0219 - val_loss: 1.0296\n",
            "Epoch 227/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0217 - val_loss: 1.0294\n",
            "Epoch 228/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0215 - val_loss: 1.0292\n",
            "Epoch 229/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0213 - val_loss: 1.0290\n",
            "Epoch 230/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0211 - val_loss: 1.0289\n",
            "Epoch 231/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0210 - val_loss: 1.0287\n",
            "Epoch 232/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0208 - val_loss: 1.0285\n",
            "Epoch 233/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0206 - val_loss: 1.0283\n",
            "Epoch 234/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0204 - val_loss: 1.0282\n",
            "Epoch 235/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0203 - val_loss: 1.0280\n",
            "Epoch 236/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0201 - val_loss: 1.0278\n",
            "Epoch 237/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0199 - val_loss: 1.0277\n",
            "Epoch 238/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0198 - val_loss: 1.0275\n",
            "Epoch 239/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0196 - val_loss: 1.0273\n",
            "Epoch 240/500\n",
            "12/12 [==============================] - 53s 4s/step - loss: 1.0195 - val_loss: 1.0272\n",
            "Epoch 241/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0193 - val_loss: 1.0270\n",
            "Epoch 242/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0191 - val_loss: 1.0269\n",
            "Epoch 243/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0190 - val_loss: 1.0267\n",
            "Epoch 244/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0188 - val_loss: 1.0266\n",
            "Epoch 245/500\n",
            "12/12 [==============================] - 52s 4s/step - loss: 1.0187 - val_loss: 1.0264\n",
            "Epoch 246/500\n",
            "12/12 [==============================] - 60s 5s/step - loss: 1.0185 - val_loss: 1.0263\n",
            "Epoch 247/500\n",
            "12/12 [==============================] - 71s 6s/step - loss: 1.0184 - val_loss: 1.0261\n",
            "Epoch 248/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0182 - val_loss: 1.0260\n",
            "Epoch 249/500\n",
            "12/12 [==============================] - 77s 6s/step - loss: 1.0181 - val_loss: 1.0258\n",
            "Epoch 250/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0180 - val_loss: 1.0257\n",
            "Epoch 251/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0178 - val_loss: 1.0256\n",
            "Epoch 252/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0177 - val_loss: 1.0254\n",
            "Epoch 253/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0175 - val_loss: 1.0253\n",
            "Epoch 254/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0174 - val_loss: 1.0251\n",
            "Epoch 255/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0173 - val_loss: 1.0250\n",
            "Epoch 256/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0171 - val_loss: 1.0249\n",
            "Epoch 257/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0170 - val_loss: 1.0248\n",
            "Epoch 258/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0169 - val_loss: 1.0246\n",
            "Epoch 259/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0168 - val_loss: 1.0245\n",
            "Epoch 260/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0166 - val_loss: 1.0244\n",
            "Epoch 261/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0165 - val_loss: 1.0243\n",
            "Epoch 262/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0164 - val_loss: 1.0241\n",
            "Epoch 263/500\n",
            "12/12 [==============================] - 74s 6s/step - loss: 1.0163 - val_loss: 1.0240\n",
            "Epoch 264/500\n",
            "12/12 [==============================] - 85s 7s/step - loss: 1.0161 - val_loss: 1.0239\n",
            "Epoch 265/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0160 - val_loss: 1.0238\n",
            "Epoch 266/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0159 - val_loss: 1.0237\n",
            "Epoch 267/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0158 - val_loss: 1.0235\n",
            "Epoch 268/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0157 - val_loss: 1.0234\n",
            "Epoch 269/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0156 - val_loss: 1.0233\n",
            "Epoch 270/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0154 - val_loss: 1.0232\n",
            "Epoch 271/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0153 - val_loss: 1.0231\n",
            "Epoch 272/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0152 - val_loss: 1.0230\n",
            "Epoch 273/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0151 - val_loss: 1.0229\n",
            "Epoch 274/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0150 - val_loss: 1.0228\n",
            "Epoch 275/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0149 - val_loss: 1.0227\n",
            "Epoch 276/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0148 - val_loss: 1.0226\n",
            "Epoch 277/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0147 - val_loss: 1.0225\n",
            "Epoch 278/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0146 - val_loss: 1.0224\n",
            "Epoch 279/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0145 - val_loss: 1.0223\n",
            "Epoch 280/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0144 - val_loss: 1.0222\n",
            "Epoch 281/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0143 - val_loss: 1.0221\n",
            "Epoch 282/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0142 - val_loss: 1.0219\n",
            "Epoch 283/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0141 - val_loss: 1.0219\n",
            "Epoch 284/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0140 - val_loss: 1.0218\n",
            "Epoch 285/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0139 - val_loss: 1.0217\n",
            "Epoch 286/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0138 - val_loss: 1.0216\n",
            "Epoch 287/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0137 - val_loss: 1.0215\n",
            "Epoch 288/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0136 - val_loss: 1.0214\n",
            "Epoch 289/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0136 - val_loss: 1.0213\n",
            "Epoch 290/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0135 - val_loss: 1.0212\n",
            "Epoch 291/500\n",
            "12/12 [==============================] - 74s 6s/step - loss: 1.0134 - val_loss: 1.0211\n",
            "Epoch 292/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0133 - val_loss: 1.0211\n",
            "Epoch 293/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0132 - val_loss: 1.0210\n",
            "Epoch 294/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0131 - val_loss: 1.0209\n",
            "Epoch 295/500\n",
            "12/12 [==============================] - 75s 6s/step - loss: 1.0130 - val_loss: 1.0208\n",
            "Epoch 296/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0130 - val_loss: 1.0207\n",
            "Epoch 297/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0129 - val_loss: 1.0206\n",
            "Epoch 298/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0128 - val_loss: 1.0206\n",
            "Epoch 299/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0127 - val_loss: 1.0205\n",
            "Epoch 300/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0126 - val_loss: 1.0204\n",
            "Epoch 301/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0125 - val_loss: 1.0203\n",
            "Epoch 302/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0125 - val_loss: 1.0202\n",
            "Epoch 303/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0124 - val_loss: 1.0202\n",
            "Epoch 304/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0123 - val_loss: 1.0201\n",
            "Epoch 305/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0122 - val_loss: 1.0200\n",
            "Epoch 306/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0122 - val_loss: 1.0199\n",
            "Epoch 307/500\n",
            "12/12 [==============================] - 88s 7s/step - loss: 1.0121 - val_loss: 1.0199\n",
            "Epoch 308/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0120 - val_loss: 1.0198\n",
            "Epoch 309/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0119 - val_loss: 1.0197\n",
            "Epoch 310/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 1.0163 - val_loss: 1.0278\n",
            "Epoch 311/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 1.0246 - val_loss: 1.0356\n",
            "Epoch 312/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0292 - val_loss: 1.0366\n",
            "Epoch 313/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 1.0274 - val_loss: 1.0332\n",
            "Epoch 314/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0243 - val_loss: 1.0290\n",
            "Epoch 315/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0225 - val_loss: 1.0304\n",
            "Epoch 316/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0222 - val_loss: 1.0291\n",
            "Epoch 317/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0192 - val_loss: 1.0251\n",
            "Epoch 318/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0171 - val_loss: 1.0252\n",
            "Epoch 319/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0172 - val_loss: 1.0238\n",
            "Epoch 320/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0161 - val_loss: 1.0223\n",
            "Epoch 321/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0153 - val_loss: 1.0221\n",
            "Epoch 322/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0141 - val_loss: 1.0131\n",
            "Epoch 323/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0125 - val_loss: 1.0136\n",
            "Epoch 324/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0124 - val_loss: 1.0150\n",
            "Epoch 325/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0122 - val_loss: 1.0102\n",
            "Epoch 326/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 1.0136 - val_loss: 1.0192\n",
            "Epoch 327/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 1.0192 - val_loss: 1.0116\n",
            "Epoch 328/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 1.0145 - val_loss: 1.0126\n",
            "Epoch 329/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 1.0120 - val_loss: 1.0133\n",
            "Epoch 330/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0119 - val_loss: 1.0132\n",
            "Epoch 331/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0091 - val_loss: 1.0097\n",
            "Epoch 332/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 1.0059 - val_loss: 1.0018\n",
            "Epoch 333/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 1.0049 - val_loss: 1.0005\n",
            "Epoch 334/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 1.0040 - val_loss: 1.0234\n",
            "Epoch 335/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0037 - val_loss: 1.0038\n",
            "Epoch 336/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0058 - val_loss: 1.0286\n",
            "Epoch 337/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 1.0003 - val_loss: 1.0467\n",
            "Epoch 338/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9982 - val_loss: 1.0619\n",
            "Epoch 339/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 1.0023 - val_loss: 1.1653\n",
            "Epoch 340/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9966 - val_loss: 1.0828\n",
            "Epoch 341/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.9906 - val_loss: 1.0194\n",
            "Epoch 342/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.9907 - val_loss: 1.0165\n",
            "Epoch 343/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.9845 - val_loss: 1.3928\n",
            "Epoch 344/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9775 - val_loss: 1.2620\n",
            "Epoch 345/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9763 - val_loss: 1.2393\n",
            "Epoch 346/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9697 - val_loss: 1.4945\n",
            "Epoch 347/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9683 - val_loss: 1.1663\n",
            "Epoch 348/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 0.9624 - val_loss: 1.0796\n",
            "Epoch 349/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9561 - val_loss: 3.8415\n",
            "Epoch 350/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9547 - val_loss: 1.5551\n",
            "Epoch 351/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9498 - val_loss: 3.0045\n",
            "Epoch 352/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9434 - val_loss: 2.3223\n",
            "Epoch 353/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9407 - val_loss: 1.8690\n",
            "Epoch 354/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9364 - val_loss: 1.3311\n",
            "Epoch 355/500\n",
            "12/12 [==============================] - 66s 6s/step - loss: 0.9341 - val_loss: 1.0761\n",
            "Epoch 356/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.9344 - val_loss: 0.9694\n",
            "Epoch 357/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.9289 - val_loss: 1.6411\n",
            "Epoch 358/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.9269 - val_loss: 2.7987\n",
            "Epoch 359/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.9259 - val_loss: 1.0851\n",
            "Epoch 360/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.9238 - val_loss: 1.0688\n",
            "Epoch 361/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.9262 - val_loss: 0.9101\n",
            "Epoch 362/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.9222 - val_loss: 1.5490\n",
            "Epoch 363/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.9204 - val_loss: 0.9125\n",
            "Epoch 364/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 0.9204 - val_loss: 0.8899\n",
            "Epoch 365/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.9184 - val_loss: 0.8979\n",
            "Epoch 366/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.9189 - val_loss: 2.8401\n",
            "Epoch 367/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.9191 - val_loss: 1.6015\n",
            "Epoch 368/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 0.9157 - val_loss: 1.6204\n",
            "Epoch 369/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 0.9184 - val_loss: 0.8920\n",
            "Epoch 370/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9162 - val_loss: 1.0969\n",
            "Epoch 371/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9177 - val_loss: 1.0255\n",
            "Epoch 372/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9162 - val_loss: 0.9159\n",
            "Epoch 373/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9154 - val_loss: 1.2107\n",
            "Epoch 374/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9148 - val_loss: 0.9015\n",
            "Epoch 375/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9134 - val_loss: 0.9807\n",
            "Epoch 376/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9117 - val_loss: 1.4399\n",
            "Epoch 377/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9091 - val_loss: 0.8948\n",
            "Epoch 378/500\n",
            "12/12 [==============================] - 71s 6s/step - loss: 0.9107 - val_loss: 0.8781\n",
            "Epoch 379/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9069 - val_loss: 0.9046\n",
            "Epoch 380/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9077 - val_loss: 1.0097\n",
            "Epoch 381/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.9081 - val_loss: 0.9069\n",
            "Epoch 382/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.9117 - val_loss: 1.0915\n",
            "Epoch 383/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.9083 - val_loss: 1.0048\n",
            "Epoch 384/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9098 - val_loss: 0.8951\n",
            "Epoch 385/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.9064 - val_loss: 0.8754\n",
            "Epoch 386/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.9028 - val_loss: 0.8929\n",
            "Epoch 387/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9030 - val_loss: 0.9180\n",
            "Epoch 388/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9042 - val_loss: 0.9148\n",
            "Epoch 389/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9020 - val_loss: 0.8795\n",
            "Epoch 390/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9027 - val_loss: 0.9231\n",
            "Epoch 391/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9022 - val_loss: 0.9820\n",
            "Epoch 392/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9031 - val_loss: 1.3695\n",
            "Epoch 393/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9017 - val_loss: 1.2141\n",
            "Epoch 394/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9006 - val_loss: 0.8815\n",
            "Epoch 395/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8997 - val_loss: 0.9078\n",
            "Epoch 396/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.8997 - val_loss: 0.8665\n",
            "Epoch 397/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.9013 - val_loss: 0.8772\n",
            "Epoch 398/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.9010 - val_loss: 0.8733\n",
            "Epoch 399/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8994 - val_loss: 0.8757\n",
            "Epoch 400/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8974 - val_loss: 0.8766\n",
            "Epoch 401/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.8977 - val_loss: 0.8717\n",
            "Epoch 402/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8990 - val_loss: 1.0219\n",
            "Epoch 403/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8949 - val_loss: 1.1451\n",
            "Epoch 404/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8928 - val_loss: 1.5181\n",
            "Epoch 405/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8965 - val_loss: 0.9181\n",
            "Epoch 406/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8956 - val_loss: 0.9116\n",
            "Epoch 407/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.8933 - val_loss: 0.8761\n",
            "Epoch 408/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.8911 - val_loss: 0.8800\n",
            "Epoch 409/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.8922 - val_loss: 0.8798\n",
            "Epoch 410/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.8922 - val_loss: 0.9891\n",
            "Epoch 411/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8904 - val_loss: 1.1473\n",
            "Epoch 412/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8898 - val_loss: 1.3490\n",
            "Epoch 413/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8903 - val_loss: 0.8821\n",
            "Epoch 414/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8893 - val_loss: 1.7397\n",
            "Epoch 415/500\n",
            "12/12 [==============================] - 74s 6s/step - loss: 0.8898 - val_loss: 0.8592\n",
            "Epoch 416/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 0.8890 - val_loss: 1.3215\n",
            "Epoch 417/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.8883 - val_loss: 0.8535\n",
            "Epoch 418/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8879 - val_loss: 0.8830\n",
            "Epoch 419/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 0.8899 - val_loss: 0.8690\n",
            "Epoch 420/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8873 - val_loss: 0.9949\n",
            "Epoch 421/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8853 - val_loss: 0.8859\n",
            "Epoch 422/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8836 - val_loss: 0.8678\n",
            "Epoch 423/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8827 - val_loss: 1.1124\n",
            "Epoch 424/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 0.8793 - val_loss: 0.8468\n",
            "Epoch 425/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8785 - val_loss: 0.9045\n",
            "Epoch 426/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8785 - val_loss: 1.0360\n",
            "Epoch 427/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8761 - val_loss: 0.9180\n",
            "Epoch 428/500\n",
            "12/12 [==============================] - 60s 5s/step - loss: 0.8759 - val_loss: 0.8593\n",
            "Epoch 429/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.8795 - val_loss: 0.8423\n",
            "Epoch 430/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8771 - val_loss: 1.1584\n",
            "Epoch 431/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8765 - val_loss: 1.1691\n",
            "Epoch 432/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8741 - val_loss: 1.0826\n",
            "Epoch 433/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.8759 - val_loss: 0.8475\n",
            "Epoch 434/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.8742 - val_loss: 1.0120\n",
            "Epoch 435/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.8729 - val_loss: 1.0290\n",
            "Epoch 436/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8725 - val_loss: 0.9351\n",
            "Epoch 437/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8732 - val_loss: 0.8447\n",
            "Epoch 438/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8742 - val_loss: 0.8812\n",
            "Epoch 439/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8739 - val_loss: 1.3854\n",
            "Epoch 440/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8716 - val_loss: 0.8622\n",
            "Epoch 441/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8747 - val_loss: 1.1240\n",
            "Epoch 442/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8721 - val_loss: 0.8587\n",
            "Epoch 443/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 0.8714 - val_loss: 0.9563\n",
            "Epoch 444/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8680 - val_loss: 0.8970\n",
            "Epoch 445/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8649 - val_loss: 0.9529\n",
            "Epoch 446/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8623 - val_loss: 0.9107\n",
            "Epoch 447/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8658 - val_loss: 0.9541\n",
            "Epoch 448/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.8641 - val_loss: 0.9130\n",
            "Epoch 449/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.8635 - val_loss: 1.1034\n",
            "Epoch 450/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.8605 - val_loss: 2.0753\n",
            "Epoch 451/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8548 - val_loss: 0.9818\n",
            "Epoch 452/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 0.8509 - val_loss: 0.8407\n",
            "Epoch 453/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.8471 - val_loss: 0.9294\n",
            "Epoch 454/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.8473 - val_loss: 0.8214\n",
            "Epoch 455/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.8437 - val_loss: 0.8858\n",
            "Epoch 456/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.8414 - val_loss: 1.0834\n",
            "Epoch 457/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8383 - val_loss: 1.4351\n",
            "Epoch 458/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8406 - val_loss: 1.0201\n",
            "Epoch 459/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8388 - val_loss: 0.9024\n",
            "Epoch 460/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8324 - val_loss: 0.8276\n",
            "Epoch 461/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8278 - val_loss: 2.1399\n",
            "Epoch 462/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.8225 - val_loss: 0.8472\n",
            "Epoch 463/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8192 - val_loss: 0.8319\n",
            "Epoch 464/500\n",
            "12/12 [==============================] - 71s 6s/step - loss: 0.8159 - val_loss: 0.8095\n",
            "Epoch 465/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8081 - val_loss: 2.4672\n",
            "Epoch 466/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8026 - val_loss: 0.9925\n",
            "Epoch 467/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.8040 - val_loss: 1.6080\n",
            "Epoch 468/500\n",
            "12/12 [==============================] - 72s 6s/step - loss: 0.8005 - val_loss: 0.7847\n",
            "Epoch 469/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.7943 - val_loss: 1.8340\n",
            "Epoch 470/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7947 - val_loss: 0.8513\n",
            "Epoch 471/500\n",
            "12/12 [==============================] - 60s 5s/step - loss: 0.7926 - val_loss: 0.8422\n",
            "Epoch 472/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.7923 - val_loss: 0.7887\n",
            "Epoch 473/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7927 - val_loss: 1.2731\n",
            "Epoch 474/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.7869 - val_loss: 1.4328\n",
            "Epoch 475/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.7838 - val_loss: 0.9823\n",
            "Epoch 476/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.7843 - val_loss: 0.9158\n",
            "Epoch 477/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 0.7790 - val_loss: 0.9890\n",
            "Epoch 478/500\n",
            "12/12 [==============================] - 60s 5s/step - loss: 0.7767 - val_loss: 1.1517\n",
            "Epoch 479/500\n",
            "12/12 [==============================] - 61s 5s/step - loss: 0.7747 - val_loss: 0.8405\n",
            "Epoch 480/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.7746 - val_loss: 0.9270\n",
            "Epoch 481/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7716 - val_loss: 0.8345\n",
            "Epoch 482/500\n",
            "12/12 [==============================] - 71s 6s/step - loss: 0.7705 - val_loss: 0.7616\n",
            "Epoch 483/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.7666 - val_loss: 0.7315\n",
            "Epoch 484/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.7652 - val_loss: 1.3017\n",
            "Epoch 485/500\n",
            "12/12 [==============================] - 73s 6s/step - loss: 0.7601 - val_loss: 0.7128\n",
            "Epoch 486/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7554 - val_loss: 0.8894\n",
            "Epoch 487/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7507 - val_loss: 0.7968\n",
            "Epoch 488/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7501 - val_loss: 0.7280\n",
            "Epoch 489/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7451 - val_loss: 0.7760\n",
            "Epoch 490/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.7447 - val_loss: 1.0378\n",
            "Epoch 491/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7391 - val_loss: 0.9134\n",
            "Epoch 492/500\n",
            "12/12 [==============================] - 62s 5s/step - loss: 0.7374 - val_loss: 1.1540\n",
            "Epoch 493/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.7329 - val_loss: 0.8400\n",
            "Epoch 494/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.7322 - val_loss: 0.8436\n",
            "Epoch 495/500\n",
            "12/12 [==============================] - 66s 5s/step - loss: 0.7301 - val_loss: 1.7740\n",
            "Epoch 496/500\n",
            "12/12 [==============================] - 65s 5s/step - loss: 0.7284 - val_loss: 0.9931\n",
            "Epoch 497/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.7271 - val_loss: 0.8549\n",
            "Epoch 498/500\n",
            "12/12 [==============================] - 63s 5s/step - loss: 0.7307 - val_loss: 1.0081\n",
            "Epoch 499/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.7279 - val_loss: 0.7714\n",
            "Epoch 500/500\n",
            "12/12 [==============================] - 64s 5s/step - loss: 0.7223 - val_loss: 1.0351\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJgkHTL9Tddn",
        "outputId": "eb8889b1-0314-4b3b-8640-212a3adf760d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Perform parameter predictions with 10,000 simulations from the test set.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load the simulations.\n",
        "x_test = np.load(\"./testSims/simModel3.npy\",mmap_mode='r')\n",
        "x_test = np.array(x_test[:,0:1000,:])\n",
        "\n",
        "# Convert the reference allele to -1.\n",
        "x_test[x_test == 0] = -1\n",
        "\n",
        "#Add missing data (coded as 0s) to the simulated matrices (with a percentage according to the empirical data - 15% in E. segueriana).\n",
        "missD = int(x_test.shape[1]*x_test.shape[2]*.15)\n",
        "for i in range(x_test.shape[0]):\n",
        "  for m in range(missD):\n",
        "    j = random.randint(0, x_test.shape[1] - 1)\n",
        "    k = random.randint(0, x_test.shape[2] - 1)\n",
        "    x_test[i][j][k] = 0\n",
        "del(missD)\n",
        "\n",
        "# Predict parameters for each simulation.\n",
        "pred = cnn.predict(x_test)\n",
        "\n",
        "# Save the obtained predictions.\n",
        "np.savetxt(\"testSet_ParameterPredictions.txt\", pred)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYZM0kNZQUfR",
        "outputId": "61092bb6-b427-4f12-9335-6dccd0cfeb45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predict parameters using the empirical data and the trained CNN.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load empirical data.\n",
        "infile=np.loadtxt(\"input_Esegueriana.txt\")\n",
        "inp=np.array(infile)\n",
        "\n",
        "# Create 100 subsets containing 1,000 random SNPs from the full empirical data.\n",
        "num_samples=100\n",
        "res = []\n",
        "for i in range(0,num_samples):\n",
        "\tidx = np.random.choice(inp.shape[0], 1000, replace=False)\n",
        "\tn = inp[idx,:]\n",
        "\tres.append(np.array(n))\n",
        "\n",
        "# Predict parameters.\n",
        "Emp_pred = np.array(res)\n",
        "Emp_pred = cnn.predict(Emp_pred)\n",
        "print(Emp_pred)\n",
        "\n",
        "np.savetxt(\"Emp_ParametersPredictions.txt\", Emp_pred)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "0PGIawyJEDo0",
        "outputId": "44d46ff4-24d4-4350-cc23-58f18322ca13"
      }
    }
  ]
}
